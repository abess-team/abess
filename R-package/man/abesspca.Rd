% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/abesspca.R
\name{abesspca}
\alias{abesspca}
\title{Adaptive best subset selection for principal component analysis}
\usage{
abesspca(
  x,
  type = c("predictor", "gram"),
  sparse.type = c("fpc", "kpc"),
  cor = FALSE,
  support.size = NULL,
  kpc.num = ifelse(sparse.type == "fpc", 1, 2),
  tune.type = c("aic", "bic", "gic", "ebic", "cv"),
  nfolds = 5,
  foldid = NULL,
  ic.scale = 1,
  c.max = NULL,
  always.include = NULL,
  group.index = NULL,
  splicing.type = 1,
  max.splicing.iter = 20,
  warm.start = TRUE,
  num.threads = 0,
  ...
)
}
\arguments{
\item{x}{A matrix object. It can be either a predictor matrix
where each row is an observation and each column is a predictor or
a sample covariance/correlation matrix.
If \code{x} is a predictor matrix, it can be in sparse matrix format
(inherit from class \code{"dgCMatrix"} in package \code{Matrix}).}

\item{type}{If \code{type = "predictor"}, \code{x} is considered as the predictor matrix.
If \code{type = "gram"}, \code{x} is considered as a sample covariance or correlation matrix.}

\item{sparse.type}{If \code{sparse.type = "fpc"}, then best subset selection performs on the first principal component;
If \code{sparse.type = "kpc"}, then best subset selection performs on the first \code{kpc.num} principal components.
(The parameter will be discard in future version.)}

\item{cor}{A logical value. If \code{cor = TRUE}, perform PCA on the correlation matrix;
otherwise, the covariance matrix.
This option is available only if \code{type = "predictor"}.
Default: \code{cor = FALSE}.}

\item{support.size}{An integer vector. It represents the alternative support sizes when \code{sparse.type = "fpc"},
while each support size controls the sparsity of a principal component when \code{sparse.type = "kpc"}.
When \code{sparse.type = "fpc"} but \code{support.size} is not supplied,
it is set as \code{support.size = 1:min(ncol(x), 100)} if \code{group.index = NULL};
otherwise, \code{support.size = 1:min(length(unique(group.index)), 100)}.
When \code{sparse.type = "kpc"} but \code{support.size} is not supplied,
then for 20\% principal components,
it is set as \code{min(ncol(x), 100)} if \code{group.index = NULL};
otherwise, \code{min(length(unique(group.index)), 100)}.}

\item{kpc.num}{A integer decide the number of components.}

\item{tune.type}{The type of criterion for choosing the support size.
Available options are \code{"gic"}, \code{"ebic"}, \code{"bic"}, \code{"aic"} and \code{"cv"}.
Default is \code{"gic"}. 
\code{tune.type = "cv"} is available only when \code{type = "predictor"}.}

\item{nfolds}{The number of folds in cross-validation. Default is \code{nfolds = 5}.}

\item{foldid}{an optional integer vector of values between 1, ..., nfolds identifying what fold each observation is in. 
The default \code{foldid = NULL} would generate a random foldid.}

\item{ic.scale}{A non-negative value used for multiplying the penalty term 
in information criterion. Default: \code{ic.scale = 1}.}

\item{c.max}{an integer splicing size. The default of \code{c.max} is the maximum of 2 and \code{max(support.size) / 2}.}

\item{always.include}{An integer vector containing the indexes of variables that should always be included in the model.}

\item{group.index}{A vector of integers indicating the which group each variable is in.
For variables in the same group, they should be located in adjacent columns of \code{x}
and their corresponding index in \code{group.index} should be the same.
Denote the first group as \code{1}, the second \code{2}, etc.
If you do not fit a model with a group structure,
please set \code{group.index = NULL} (the default).}

\item{splicing.type}{Optional type for splicing.
If \code{splicing.type = 1}, the number of variables to be spliced is
\code{c.max}, ..., \code{1}; if \code{splicing.type = 2},
the number of variables to be spliced is \code{c.max}, \code{c.max/2}, ..., \code{1}.
Default: \code{splicing.type = 1}.}

\item{max.splicing.iter}{The maximum number of performing splicing algorithm.
In most of the case, only a few times of splicing iteration can guarantee the convergence.
Default is \code{max.splicing.iter = 20}.}

\item{warm.start}{Whether to use the last solution as a warm start. Default is \code{warm.start = TRUE}.}

\item{num.threads}{An integer decide the number of threads to be
concurrently used for cross-validation (i.e., \code{tune.type = "cv"}).
If \code{num.threads = 0}, then all of available cores will be used.
Default: \code{num.threads = 0}.}

\item{...}{further arguments to be passed to or from methods.}
}
\value{
A S3 \code{abesspca} class object, which is a \code{list} with the following components:
\item{coef}{A \eqn{p}-by-\code{length(support.size)} loading matrix of sparse principal components (PC),
where each row is a variable and each column is a support size;}
\item{nvars}{The number of variables.}
\item{sparse.type}{The same as input.}
\item{support.size}{The actual support.size values used. Note that it is not necessary the same as the input if the later have non-integer values or duplicated values.}
\item{ev}{A vector with size \code{length(support.size)}. It records the explained variance at each support size.}
\item{cum.ev}{Cumulative sums of explained variance.}
\item{pev}{A vector with the same length as \code{ev}. It records the percentage of explained variance at each support size.}
\item{cum.pev}{Cumulative sums of the percentage of explained variance.}
\item{var.all}{If \code{sparse.type = "fpc"},
it is the total variance of the explained by first principal component;
otherwise, the total standard deviations of all principal components.}
\item{call}{The original call to \code{abess}.}
}
\description{
Adaptive best subset selection for principal component analysis
}
\details{
Adaptive best subset selection for principal component analysis aim
to solve the non-convex optimization problem:
\deqn{-\arg\min_{v} v^\top \Sigma v, s.t.\quad v^\top v=1, \|v\|_0 \leq s, }
where \eqn{s} is support size. 
Here, \eqn{\Sigma} is covariance matrix, i.e., 
\deqn{\Sigma = \frac{1}{n} X^{\top} X.}
A generic splicing technique is implemented to
solve this problem.
By exploiting the warm-start initialization, the non-convex optimization
problem at different support size (specified by \code{support.size})
can be efficiently solved.
}
\examples{
\donttest{
library(abess)

## predictor matrix input:
head(USArrests)
pca_fit <- abesspca(USArrests)
pca_fit

## covariance matrix input:
cov_mat <- stats::cov(USArrests) * (nrow(USArrests) - 1) / nrow(USArrests)
pca_fit <- abesspca(cov_mat, type = "gram")
pca_fit  

## robust covariance matrix input:
rob_cov <- MASS::cov.rob(USArrests)[["cov"]]
rob_cov <- (rob_cov + t(rob_cov)) / 2
pca_fit <- abesspca(rob_cov, type = "gram")
pca_fit

## select support size via cross-validation ##
pca_fit <- abesspca(USArrests, tune.type = "cv")
pca_fit

## K-component principal component analysis
pca_fit <- abesspca(USArrests,
  sparse.type = "kpc",
  support.size = c(1, 2)
)
coef(pca_fit)

}
}
\references{
A polynomial algorithm for best-subset selection problem. Junxian Zhu, Canhong Wen, Jin Zhu, Heping Zhang, Xueqin Wang. Proceedings of the National Academy of Sciences Dec 2020, 117 (52) 33117-33123; DOI: 10.1073/pnas.2014241117

Sparse principal component analysis. Hui Zou, Hastie Trevor, and Tibshirani Robert. Journal of computational and graphical statistics 15.2 (2006): 265-286.
}
\seealso{
\code{\link{print.abesspca}},
\code{\link{coef.abesspca}},
\code{\link{plot.abesspca}}.
}
\author{
Jin Zhu, Junxian Zhu, Ruihuang Liu, Junhao Huang, Xueqin Wang
}
