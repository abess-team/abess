<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Principal component analysis • abess</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.min.js" integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/cerulean/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><!-- docsearch --><script src="../docsearch.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/docsearch.js/2.6.3/docsearch.min.css" integrity="sha256-QOSRU/ra9ActyXkIBbiIB144aDBdtvXBcNc3OTNuX/Q=" crossorigin="anonymous">
<link href="../docsearch.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script><meta property="og:title" content="Principal component analysis">
<meta property="og:description" content="abess">
<meta property="og:image" content="https://abess-team.github.io/abess/logo.svg">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">abess</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">0.4.8</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/v01-abess-guide.html">Quick start for `abess`: Linear regression</a>
    </li>
    <li>
      <a href="../articles/v03-classification.html">Classification: Logistic Regression and Multinomial Extension</a>
    </li>
    <li>
      <a href="../articles/v04-PoissonGammaReg.html">Positive response: Poisson and Gamma regression</a>
    </li>
    <li>
      <a href="../articles/v05-coxreg.html">Best Subset Selection for Censored Response</a>
    </li>
    <li>
      <a href="../articles/v06-MultiTaskLearning.html">Multi-Response Linear Regression</a>
    </li>
    <li>
      <a href="../articles/v07-advancedFeatures.html">Advanced Features</a>
    </li>
    <li>
      <a href="../articles/v08-sPCA.html">Principal component analysis</a>
    </li>
    <li>
      <a href="../articles/v09-fasterSetting.html">Tips for faster computation</a>
    </li>
    <li>
      <a href="../articles/v10-algorithm.html">ABESS algorithm: details</a>
    </li>
    <li>
      <a href="../articles/v11-power-of-abess.html">Power of abess</a>
    </li>
    <li>
      <a href="../articles/v12-Robust-Principal-Component-Analysis.html">Robust Principal Component Analysis</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/abess-team/abess/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
<form class="navbar-form navbar-right hidden-xs hidden-sm" role="search">
        <div class="form-group">
          <input type="search" class="form-control" name="search-input" id="search-input" placeholder="Search..." aria-label="Search for..." autocomplete="off">
</div>
      </form>
      
    </div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Principal component analysis</h1>
                        <h4 data-toc-skip class="author">Junhao Huang,
Jin Zhu</h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/abess-team/abess/tree/master/R-package/../vignettes/v08-sPCA.Rmd" class="external-link"><code>../vignettes/v08-sPCA.Rmd</code></a></small>
      <div class="hidden name"><code>v08-sPCA.Rmd</code></div>

    </div>

    
    
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Sys.setenv.html" class="external-link">Sys.setenv</a></span><span class="op">(</span><span class="st">"OMP_THREAD_LIMIT"</span> <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>Principal component analysis (PCA) is an important method in the
field of data science, which can reduce the dimension of data and
simplify our model. It actually solve an optimization problem like:</p>
<p><span class="math display">\[
    \max_{v} v^{\top}\Sigma v,\qquad s.t.\quad v^{\top}v=1.
\]</span></p>
<p>where <span class="math inline">\(\Sigma = X^TX / (n-1)\)</span> and
<span class="math inline">\(X\)</span> is the <strong>centered</strong>
sample matrix. We also denote that <span class="math inline">\(X\)</span> is a <span class="math inline">\(n\times p\)</span> matrix, where each row is an
observation and each column is a variable.</p>
<p>Then, before further analysis, we can project <span class="math inline">\(X\)</span> to <span class="math inline">\(v\)</span> (thus dimensional reduction), without
losing too much information.</p>
<p>However, consider that:</p>
<ul>
<li><p>The PC is a linear combination of all primary variables (<span class="math inline">\(Xv\)</span>), but sometimes we may tend to use
less variables for clearer interpretation (and less computational
complexity);</p></li>
<li><p>It has been proved that if <span class="math inline">\(p/n\)</span> does not converge to <span class="math inline">\(0\)</span>, the classical PCA is not consistent,
but this would happen in some high-dimensional data analysis.</p></li>
</ul>
<p>For example, in gene analysis, the dataset may contain plenty of
genes (variables) and we would like to find a subset of them, which can
explain most information. Compared with using all genes, this small
subset may perform better on interpretation, without loss much
information. Then we can focus on these variables in the further
analysis.</p>
<p>When we are trapped by these problems, a classical PCA may not be a
best choice, since it use all variables. One of the alternatives is
abessPCA, which is able to seek for principal component with a sparsity
limitation:</p>
<p><span class="math display">\[
    \max_{v} v^{\top}\Sigma v,\qquad s.t.\quad v^{\top}v=1,\ ||v||_0\leq
s.
\]</span></p>
<p>where <span class="math inline">\(s\)</span> is a non-negative
integer, which indicates how many primary variables are used in
principal component. With abessPCA, we can search for the best subset of
variables to form principal component and it retains consistency even
under <span class="math inline">\(p&gt;&gt;n\)</span>. And we make two
remarks:</p>
<ul>
<li>Clearly, if <span class="math inline">\(s\)</span> is equal or
larger than the number of primary variables, this sparsity limitation is
actually useless, so the problem is equivalent to a classical PCA.</li>
<li>With less variables, the PC must have lower explained variance.
However, this decrease is slight if we choose a good <span class="math inline">\(s\)</span> and at this price, we can interpret the
PC much better. It is worthy.</li>
</ul>
<p>In the next section, we will show how to form abessPCA in our
frame.</p>
</div>
<div class="section level2">
<h2 id="abesspca-real-data-example">abessPCA: real data example<a class="anchor" aria-label="anchor" href="#abesspca-real-data-example"></a>
</h2>
<div class="section level3">
<h3 id="communities-and-crime-dataset">Communities-and-crime dataset<a class="anchor" aria-label="anchor" href="#communities-and-crime-dataset"></a>
</h3>
<p>Here we will use real data analysis to show how to form abessPCA. The
data we use is from <a href="https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime" class="external-link">UCI:
Communities and Crime Data Set</a>.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html" class="external-link">read.csv</a></span><span class="op">(</span><span class="st">'./communities.data'</span>, header <span class="op">=</span> <span class="cn">FALSE</span>, na.strings <span class="op">=</span> <span class="st">'?'</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/dim.html" class="external-link">dim</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 1994  128</span></span></code></pre>
<p>The dataset contain 128 variables but a part of them have missing
values or categorical variables. We simply drop these variables, and
retain 99 predictive variables as our data example.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">X</span> <span class="op">&lt;-</span> <span class="va">X</span><span class="op">[</span>, <span class="fl">6</span><span class="op">:</span><span class="fl">127</span><span class="op">]</span></span>
<span><span class="va">na_col</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html" class="external-link">apply</a></span><span class="op">(</span><span class="va">X</span>, <span class="fl">2</span>, <span class="va">anyNA</span><span class="op">)</span></span>
<span><span class="va">X</span> <span class="op">&lt;-</span> <span class="va">X</span><span class="op">[</span>, <span class="op">!</span><span class="va">na_col</span><span class="op">]</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/dim.html" class="external-link">dim</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 1994   99</span></span></code></pre>
</div>
<div class="section level3">
<h3 id="adaptive-best-subset-selection-for-pca">Adaptive best subset selection for PCA<a class="anchor" aria-label="anchor" href="#adaptive-best-subset-selection-for-pca"></a>
</h3>
<p>Next, we turn to fit abessPCA. For fitting the model, we can give
either predictor matrix <span class="math inline">\(X\)</span>:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/abess-team/abess" class="external-link">abess</a></span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">##  Thank you for using abess! To acknowledge our work, please cite the package:</span></span></code></pre>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">##  Zhu J, Wang X, Hu L, Huang J, Jiang K, Zhang Y, Lin S, Zhu J (2022). 'abess: A Fast Best Subset Selection Library in Python and R.' Journal of Machine Learning Research, 23(202), 1-7. https://www.jmlr.org/papers/v23/21-1060.html.</span></span></code></pre>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">best_pca</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/abesspca.html">abesspca</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/str.html" class="external-link">str</a></span><span class="op">(</span><span class="va">best_pca</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## List of 15</span></span>
<span><span class="co">##  $ coef        :Formal class 'dgCMatrix' [package "Matrix"] with 6 slots</span></span>
<span><span class="co">##   .. ..@ i       : int [1:4950] 70 10 70 11 76 83 11 76 81 83 ...</span></span>
<span><span class="co">##   .. ..@ p       : int [1:100] 0 1 3 6 10 15 21 28 36 45 ...</span></span>
<span><span class="co">##   .. ..@ Dim     : int [1:2] 99 99</span></span>
<span><span class="co">##   .. ..@ Dimnames:List of 2</span></span>
<span><span class="co">##   .. .. ..$ : chr [1:99] "V6" "V7" "V8" "V9" ...</span></span>
<span><span class="co">##   .. .. ..$ : chr [1:99] "1" "2" "3" "4" ...</span></span>
<span><span class="co">##   .. ..@ x       : num [1:4950] 1 -0.641 -0.767 -0.899 0.31 ...</span></span>
<span><span class="co">##   .. ..@ factors : list()</span></span>
<span><span class="co">##  $ tune.value  : num [1:99] 0 0 0 0 0 0 0 0 0 0 ...</span></span>
<span><span class="co">##  $ ev          : num [1:99, 1] 0.0226 0.0369 0.228 0.2487 0.2736 ...</span></span>
<span><span class="co">##  $ kpc.num     : num 1</span></span>
<span><span class="co">##  $ var.pc      : num [1:99] 1.065 0.752 0.33 0.284 0.184 ...</span></span>
<span><span class="co">##  $ cum.var.pc  : num [1:99] 1.07 1.82 2.15 2.43 2.62 ...</span></span>
<span><span class="co">##  $ var.all     : num 3.97</span></span>
<span><span class="co">##  $ pev         : num [1:99, 1] 0.00571 0.00931 0.05748 0.0627 0.06898 ...</span></span>
<span><span class="co">##  $ pev.pc      : num [1:99, 1] 0.0212 0.0347 0.214 0.2335 0.2568 ...</span></span>
<span><span class="co">##  $ nvars       : int 99</span></span>
<span><span class="co">##  $ sparse.type : chr "fpc"</span></span>
<span><span class="co">##  $ support.size: num [1:99] 1 2 3 4 5 6 7 8 9 10 ...</span></span>
<span><span class="co">##  $ tune.type   : chr "gic"</span></span>
<span><span class="co">##  $ tune.path   : chr "sequence"</span></span>
<span><span class="co">##  $ call        : language abesspca(x = X)</span></span>
<span><span class="co">##  - attr(*, "class")= chr "abesspca"</span></span></code></pre>
<p>or Gram-type matrix (like covariance matrix, correlation matrix and
robust covariance matrix):</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">best_pca</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/abesspca.html">abesspca</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html" class="external-link">cov</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span>, type <span class="op">=</span> <span class="st">"gram"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/str.html" class="external-link">str</a></span><span class="op">(</span><span class="va">best_pca</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## List of 15</span></span>
<span><span class="co">##  $ coef        :Formal class 'dgCMatrix' [package "Matrix"] with 6 slots</span></span>
<span><span class="co">##   .. ..@ i       : int [1:4950] 70 10 70 11 76 83 11 76 81 83 ...</span></span>
<span><span class="co">##   .. ..@ p       : int [1:100] 0 1 3 6 10 15 21 28 36 45 ...</span></span>
<span><span class="co">##   .. ..@ Dim     : int [1:2] 99 99</span></span>
<span><span class="co">##   .. ..@ Dimnames:List of 2</span></span>
<span><span class="co">##   .. .. ..$ : chr [1:99] "V6" "V7" "V8" "V9" ...</span></span>
<span><span class="co">##   .. .. ..$ : chr [1:99] "1" "2" "3" "4" ...</span></span>
<span><span class="co">##   .. ..@ x       : num [1:4950] 1 -0.641 -0.767 -0.899 0.31 ...</span></span>
<span><span class="co">##   .. ..@ factors : list()</span></span>
<span><span class="co">##  $ tune.value  : num [1:99] 0 0 0 0 0 0 0 0 0 0 ...</span></span>
<span><span class="co">##  $ ev          : num [1:99, 1] 0.0226 0.037 0.2281 0.2488 0.2739 ...</span></span>
<span><span class="co">##  $ kpc.num     : num 1</span></span>
<span><span class="co">##  $ var.pc      : num [1:99] 1.066 0.752 0.33 0.284 0.185 ...</span></span>
<span><span class="co">##  $ cum.var.pc  : num [1:99] 1.07 1.82 2.15 2.43 2.62 ...</span></span>
<span><span class="co">##  $ var.all     : num 3.97</span></span>
<span><span class="co">##  $ pev         : num [1:99, 1] 0.00571 0.00931 0.05748 0.0627 0.06901 ...</span></span>
<span><span class="co">##  $ pev.pc      : num [1:99, 1] 0.0212 0.0347 0.214 0.2335 0.257 ...</span></span>
<span><span class="co">##  $ nvars       : int 99</span></span>
<span><span class="co">##  $ sparse.type : chr "fpc"</span></span>
<span><span class="co">##  $ support.size: num [1:99] 1 2 3 4 5 6 7 8 9 10 ...</span></span>
<span><span class="co">##  $ tune.type   : chr "gic"</span></span>
<span><span class="co">##  $ tune.path   : chr "sequence"</span></span>
<span><span class="co">##  $ call        : language abesspca(x = cov(X), type = "gram")</span></span>
<span><span class="co">##  - attr(*, "class")= chr "abesspca"</span></span></code></pre>
</div>
<div class="section level3">
<h3 id="interpreting-result">Interpreting result<a class="anchor" aria-label="anchor" href="#interpreting-result"></a>
</h3>
<p>After fitting abessPCA, we study the percentage of explained variance
as <span class="math inline">\(s\)</span> increases:</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">best_pca</span><span class="op">[[</span><span class="st">"support.size"</span><span class="op">]</span><span class="op">]</span>, <span class="va">best_pca</span><span class="op">[[</span><span class="st">"pev"</span><span class="op">]</span><span class="op">]</span>, type <span class="op">=</span> <span class="st">"l"</span><span class="op">)</span></span></code></pre></div>
<p><img src="v08-sPCA_files/figure-html/unnamed-chunk-6-1.png" width="700"></p>
<p>It is clear that the higher sparsity is, the more variance it can
explain. Interestingly, we can seek for a smaller sparsity which can
explain most of the variance. For instance, when 40 variables are
selected, the percentage of explained variance from abessPCA exceeds
80%.<br>
This result shows that using less than half of all 99 variables can be
close to perfect. We can use <code>coef</code> function to investigate
which variables are selected when the explained variance are large. For
example, if we choose sparsity 40, the used variables are:</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html" class="external-link">coef</a></span><span class="op">(</span><span class="va">best_pca</span>, support.size <span class="op">=</span> <span class="fl">40</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## 99 x 1 sparse Matrix of class "dgCMatrix"</span></span>
<span><span class="co">##              40</span></span>
<span><span class="co">## V6    .        </span></span>
<span><span class="co">## V7    .        </span></span>
<span><span class="co">## V8   -0.1365515</span></span>
<span><span class="co">## V9    0.1227176</span></span>
<span><span class="co">## V10   .        </span></span>
<span><span class="co">## V11   .        </span></span>
<span><span class="co">## V12   .        </span></span>
<span><span class="co">## V13   .        </span></span>
<span><span class="co">## V14   .        </span></span>
<span><span class="co">## V15   .        </span></span>
<span><span class="co">## V16   .        </span></span>
<span><span class="co">## V17   0.1745626</span></span>
<span><span class="co">## V18   0.1999679</span></span>
<span><span class="co">## V19   0.1117617</span></span>
<span><span class="co">## V20   .        </span></span>
<span><span class="co">## V21   0.1559536</span></span>
<span><span class="co">## V22   .        </span></span>
<span><span class="co">## V23  -0.1808407</span></span>
<span><span class="co">## V24   .        </span></span>
<span><span class="co">## V25   0.1912228</span></span>
<span><span class="co">## V26   0.1715435</span></span>
<span><span class="co">## V27   0.1505258</span></span>
<span><span class="co">## V28   0.1140740</span></span>
<span><span class="co">## V29   .        </span></span>
<span><span class="co">## V30   .        </span></span>
<span><span class="co">## V32   0.1174763</span></span>
<span><span class="co">## V33   .        </span></span>
<span><span class="co">## V34  -0.1981478</span></span>
<span><span class="co">## V35  -0.1561935</span></span>
<span><span class="co">## V36  -0.1673926</span></span>
<span><span class="co">## V37   0.1551137</span></span>
<span><span class="co">## V38  -0.1588460</span></span>
<span><span class="co">## V39   0.1167834</span></span>
<span><span class="co">## V40   .        </span></span>
<span><span class="co">## V41   .        </span></span>
<span><span class="co">## V42  -0.1451945</span></span>
<span><span class="co">## V43   0.1448450</span></span>
<span><span class="co">## V44  -0.1137527</span></span>
<span><span class="co">## V45   .        </span></span>
<span><span class="co">## V46  -0.1069923</span></span>
<span><span class="co">## V47  -0.1164267</span></span>
<span><span class="co">## V48   .        </span></span>
<span><span class="co">## V49   0.1697180</span></span>
<span><span class="co">## V50   0.1756634</span></span>
<span><span class="co">## V51   0.1881001</span></span>
<span><span class="co">## V52   0.1389346</span></span>
<span><span class="co">## V53   .        </span></span>
<span><span class="co">## V54   .        </span></span>
<span><span class="co">## V55   .        </span></span>
<span><span class="co">## V56  -0.1542476</span></span>
<span><span class="co">## V57   .        </span></span>
<span><span class="co">## V58   .        </span></span>
<span><span class="co">## V59   .        </span></span>
<span><span class="co">## V60   .        </span></span>
<span><span class="co">## V61   .        </span></span>
<span><span class="co">## V62   .        </span></span>
<span><span class="co">## V63   .        </span></span>
<span><span class="co">## V64   .        </span></span>
<span><span class="co">## V65   .        </span></span>
<span><span class="co">## V66   .        </span></span>
<span><span class="co">## V67   .        </span></span>
<span><span class="co">## V68   .        </span></span>
<span><span class="co">## V69   .        </span></span>
<span><span class="co">## V70   .        </span></span>
<span><span class="co">## V71   .        </span></span>
<span><span class="co">## V72   .        </span></span>
<span><span class="co">## V73   0.1208047</span></span>
<span><span class="co">## V74   .        </span></span>
<span><span class="co">## V75   .        </span></span>
<span><span class="co">## V76   0.1056635</span></span>
<span><span class="co">## V77   .        </span></span>
<span><span class="co">## V78   .        </span></span>
<span><span class="co">## V79   .        </span></span>
<span><span class="co">## V80  -0.1118632</span></span>
<span><span class="co">## V81   .        </span></span>
<span><span class="co">## V82   .        </span></span>
<span><span class="co">## V83  -0.2118385</span></span>
<span><span class="co">## V84  -0.1127914</span></span>
<span><span class="co">## V85   0.1789415</span></span>
<span><span class="co">## V86   0.1819897</span></span>
<span><span class="co">## V87   0.1818463</span></span>
<span><span class="co">## V88   0.1831991</span></span>
<span><span class="co">## V89   0.1793673</span></span>
<span><span class="co">## V90   0.2122642</span></span>
<span><span class="co">## V91   0.1823010</span></span>
<span><span class="co">## V92   .        </span></span>
<span><span class="co">## V93   .        </span></span>
<span><span class="co">## V94   .        </span></span>
<span><span class="co">## V95   .        </span></span>
<span><span class="co">## V96   .        </span></span>
<span><span class="co">## V97   .        </span></span>
<span><span class="co">## V98   .        </span></span>
<span><span class="co">## V99   .        </span></span>
<span><span class="co">## V100  .        </span></span>
<span><span class="co">## V101  .        </span></span>
<span><span class="co">## V119  .        </span></span>
<span><span class="co">## V120  .        </span></span>
<span><span class="co">## V121  .        </span></span>
<span><span class="co">## V126  .</span></span></code></pre>
<p>where each row of loading matrix corresponds to a variable.</p>
</div>
</div>
<div class="section level2">
<h2 id="extension">Extension<a class="anchor" aria-label="anchor" href="#extension"></a>
</h2>
<div class="section level3">
<h3 id="group-variable">Group variable<a class="anchor" aria-label="anchor" href="#group-variable"></a>
</h3>
<p>In some cases, some variables may need to consider together, that is,
they should be “used” or “unused” for PC at the same time, which we call
“group information”. The optimization problem becomes:</p>
<p><span class="math display">\[
    \max_{v} v^{\top}\Sigma v,\qquad s.t.\quad v^{\top}v=1,\
\sum_{g=1}^G I(||v_g||\neq 0)\leq s.
\]</span></p>
<p>where we suppose there are <span class="math inline">\(G\)</span>
groups, and the <span class="math inline">\(g\)</span>-th one correspond
to <span class="math inline">\(v_g\)</span>, <span class="math inline">\(v = [v_1^T,v_2^T,\cdots,v_G^T]^T\)</span>. Then we
are interested to find <span class="math inline">\(s\)</span> (or less)
important groups.</p>
<p>Group problem is extraordinary important in real data analysis. Still
take gene analysis as an example, several sites would be related to one
pathway, and it is meaningless to consider each of them alone.</p>
<p>abessPCA can also deal with group information. Here we make sure that
variables in the same group address close to each other (if not, the
data should be sorted first).</p>
<p>Suppose that the data above have group information like:</p>
<ul>
<li>Group 1: {the 1st, 2nd, …, 6th variable};</li>
<li>Group 2: {the 7th, 8th, …, 12th variable};</li>
<li>…</li>
<li>Group 16: {the 91st, 92nd, …, 96th variable};</li>
<li>Group 17: {the 97th, 98th, 99th variables}.</li>
</ul>
<p>Denote different groups as different number:</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">g_info</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">16</span>, each <span class="op">=</span> <span class="fl">6</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="fl">17</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>And fit a group SPCA model with additional argument
<code>group.index = g_info</code>:</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">best_pca</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/abesspca.html">abesspca</a></span><span class="op">(</span><span class="va">X</span>, group.index <span class="op">=</span> <span class="va">g_info</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/str.html" class="external-link">str</a></span><span class="op">(</span><span class="va">best_pca</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## List of 15</span></span>
<span><span class="co">##  $ coef        :Formal class 'dgCMatrix' [package "Matrix"] with 6 slots</span></span>
<span><span class="co">##   .. ..@ i       : int [1:912] 42 43 44 45 46 47 18 19 20 21 ...</span></span>
<span><span class="co">##   .. ..@ p       : int [1:18] 0 6 18 36 60 90 126 168 216 270 ...</span></span>
<span><span class="co">##   .. ..@ Dim     : int [1:2] 99 17</span></span>
<span><span class="co">##   .. ..@ Dimnames:List of 2</span></span>
<span><span class="co">##   .. .. ..$ : chr [1:99] "V6" "V7" "V8" "V9" ...</span></span>
<span><span class="co">##   .. .. ..$ : chr [1:17] "1" "2" "3" "4" ...</span></span>
<span><span class="co">##   .. ..@ x       : num [1:912] -0.503 -0.512 -0.5279 -0.4516 0.0472 ...</span></span>
<span><span class="co">##   .. ..@ factors : list()</span></span>
<span><span class="co">##  $ tune.value  : num [1:17] 0 0 0 0 0 0 0 0 0 0 ...</span></span>
<span><span class="co">##  $ ev          : num [1:17, 1] 0.158 0.388 0.471 0.561 0.645 ...</span></span>
<span><span class="co">##  $ kpc.num     : num 1</span></span>
<span><span class="co">##  $ var.pc      : num [1:99] 1.065 0.752 0.33 0.284 0.184 ...</span></span>
<span><span class="co">##  $ cum.var.pc  : num [1:99] 1.07 1.82 2.15 2.43 2.62 ...</span></span>
<span><span class="co">##  $ var.all     : num 3.97</span></span>
<span><span class="co">##  $ pev         : num [1:17, 1] 0.0399 0.0979 0.1188 0.1415 0.1625 ...</span></span>
<span><span class="co">##  $ pev.pc      : num [1:17, 1] 0.148 0.364 0.442 0.527 0.605 ...</span></span>
<span><span class="co">##  $ nvars       : int 99</span></span>
<span><span class="co">##  $ sparse.type : chr "fpc"</span></span>
<span><span class="co">##  $ support.size: num [1:17] 1 2 3 4 5 6 7 8 9 10 ...</span></span>
<span><span class="co">##  $ tune.type   : chr "gic"</span></span>
<span><span class="co">##  $ tune.path   : chr "sequence"</span></span>
<span><span class="co">##  $ call        : language abesspca(x = X, group.index = g_info)</span></span>
<span><span class="co">##  - attr(*, "class")= chr "abesspca"</span></span></code></pre>
</div>
<div class="section level3">
<h3 id="multiple-principal-components">Multiple principal components<a class="anchor" aria-label="anchor" href="#multiple-principal-components"></a>
</h3>
<p>In some cases, we may seek for more than one principal components
under sparsity. Actually, we can iteratively solve the largest principal
component and then mapping the covariance matrix to its orthogonal
space:</p>
<p><span class="math display">\[
\Sigma' = (1-vv^{\top})\Sigma(1-vv^{\top})
\]</span></p>
<p>where <span class="math inline">\(\Sigma\)</span> is the covariance
matrix and <span class="math inline">\(v\)</span> is its (sparse)
loading vector. We map it into <span class="math inline">\(\Sigma'\)</span>, which indicates the
orthogonal space of <span class="math inline">\(v\)</span>, and then
solve the sparse principal component for <span class="math inline">\(\Sigma'\)</span> again. By this iteration
process, we can acquire multiple principal components and they are
sorted from the largest to the smallest. In our program, there is an
additional argument <code>sparse.type</code> to support this feature. By
setting <code>sparse.type = "kpc"</code>, then best subset selection
performs on the first <span class="math inline">\(K\)</span> principal
components where <span class="math inline">\(K\)</span> is decided by
argument <code>support.size</code>.</p>
<p>Suppose we are interested in the first two principal components, and
the support size is 50 in the first loading vector and is 40 in the
second loading vector. In other words, we consecutively solve two
problem: <span class="math display">\[
    v_1 \leftarrow \arg\max_{v} v^{\top}\Sigma v,\qquad s.t.\quad
v^{\top}v=1,\ ||v||_0\leq 10,
\]</span> <span class="math display">\[
  v_2 \leftarrow \arg\max_{v} v^{\top} \Sigma^{\prime} v,\qquad
s.t.\quad v^{\top}v=1,\ ||v||_0\leq 5,
\]</span> where <span class="math inline">\(\Sigma^{\prime} = (1-v_1
v_1^\top)\Sigma(1-v_1 v_1^\top)\)</span>. The <span class="math inline">\((v_1, v_2)\)</span> forms a sparse loading
matrix.</p>
<p>The code for solving the two problem is:</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">best_kpca</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/abesspca.html">abesspca</a></span><span class="op">(</span><span class="va">X</span>, support.size <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">50</span>, <span class="fl">40</span><span class="op">)</span>, sparse.type <span class="op">=</span> <span class="st">"kpc"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/str.html" class="external-link">str</a></span><span class="op">(</span><span class="va">best_kpca</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## List of 15</span></span>
<span><span class="co">##  $ coef        :List of 2</span></span>
<span><span class="co">##   ..$ :Formal class 'dgCMatrix' [package "Matrix"] with 6 slots</span></span>
<span><span class="co">##   .. .. ..@ i       : int [1:90] 2 3 11 12 13 15 17 19 20 21 ...</span></span>
<span><span class="co">##   .. .. ..@ p       : int [1:3] 0 40 90</span></span>
<span><span class="co">##   .. .. ..@ Dim     : int [1:2] 99 2</span></span>
<span><span class="co">##   .. .. ..@ Dimnames:List of 2</span></span>
<span><span class="co">##   .. .. .. ..$ : chr [1:99] "V6" "V7" "V8" "V9" ...</span></span>
<span><span class="co">##   .. .. .. ..$ : chr [1:2] "40" "50"</span></span>
<span><span class="co">##   .. .. ..@ x       : num [1:90] -0.136 0.123 0.175 0.2 0.111 ...</span></span>
<span><span class="co">##   .. .. ..@ factors : list()</span></span>
<span><span class="co">##   ..$ :Formal class 'dgCMatrix' [package "Matrix"] with 6 slots</span></span>
<span><span class="co">##   .. .. ..@ i       : int [1:90] 1 3 4 5 11 17 38 41 49 51 ...</span></span>
<span><span class="co">##   .. .. ..@ p       : int [1:3] 0 40 90</span></span>
<span><span class="co">##   .. .. ..@ Dim     : int [1:2] 99 2</span></span>
<span><span class="co">##   .. .. ..@ Dimnames:List of 2</span></span>
<span><span class="co">##   .. .. .. ..$ : chr [1:99] "V6" "V7" "V8" "V9" ...</span></span>
<span><span class="co">##   .. .. .. ..$ : chr [1:2] "40" "50"</span></span>
<span><span class="co">##   .. .. ..@ x       : num [1:90] 0.071 -0.171 0.159 0.211 0.171 ...</span></span>
<span><span class="co">##   .. .. ..@ factors : list()</span></span>
<span><span class="co">##  $ ev          :List of 2</span></span>
<span><span class="co">##   ..$ : num [1:2, 1] 0.955 1.015</span></span>
<span><span class="co">##   ..$ : num [1:2] 1.75 1.77</span></span>
<span><span class="co">##  $ tune.value  :List of 2</span></span>
<span><span class="co">##   ..$ : num [1:2, 1] 371 464</span></span>
<span><span class="co">##   ..$ : num [1:2, 1] 371 464</span></span>
<span><span class="co">##  $ kpc.num     : num 2</span></span>
<span><span class="co">##  $ var.pc      : num [1:99] 1.065 0.752 0.33 0.284 0.184 ...</span></span>
<span><span class="co">##  $ cum.var.pc  : num [1:99] 1.07 1.82 2.15 2.43 2.62 ...</span></span>
<span><span class="co">##  $ var.all     : num 3.97</span></span>
<span><span class="co">##  $ pev         :List of 2</span></span>
<span><span class="co">##   ..$ : num [1:2, 1] 0.241 0.256</span></span>
<span><span class="co">##   ..$ : num [1:2] 0.441 0.446</span></span>
<span><span class="co">##  $ pev.pc      :List of 2</span></span>
<span><span class="co">##   ..$ : num [1:2, 1] 0.897 0.953</span></span>
<span><span class="co">##   ..$ : num [1:2] 0.962 0.972</span></span>
<span><span class="co">##  $ nvars       : int 99</span></span>
<span><span class="co">##  $ sparse.type : chr "kpc"</span></span>
<span><span class="co">##  $ support.size:List of 2</span></span>
<span><span class="co">##   ..$ : num [1:2] 40 50</span></span>
<span><span class="co">##   ..$ : num [1:2] 40 50</span></span>
<span><span class="co">##  $ tune.type   : chr "gic"</span></span>
<span><span class="co">##  $ tune.path   : chr "sequence"</span></span>
<span><span class="co">##  $ call        : language abesspca(x = X, sparse.type = "kpc", support.size = c(50, 40))</span></span>
<span><span class="co">##  - attr(*, "class")= chr "abesspca"</span></span></code></pre>
<p>The result <code>best_kpca[["pev"]]</code> shows that two principal
components raised from two loading matrix could explain 40% variance of
all variables (i.e., <span class="math inline">\(\text{trace}(\Sigma)\)</span>).</p>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Jin Zhu, Zezhi Wang, Liyuan Hu, Junhao Huang, Kangkang Jiang, Yanhang Zhang, Borui Tang, Shiyun Lin, Junxian Zhu, Canhong Wen, Heping Zhang, Xueqin Wang.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.9.</p>
</div>

      </footer>
</div>

  
<script src="https://cdnjs.cloudflare.com/ajax/libs/docsearch.js/2.6.1/docsearch.min.js" integrity="sha256-GKvGqXDznoRYHCwKXGnuchvKSwmx9SRMrZOTh2g4Sb0=" crossorigin="anonymous"></script><script>
  docsearch({
    
    
    apiKey: 'd32715b0e35635336aba6377dd751e21',
    indexName: 'abess',
    inputSelector: 'input#search-input.form-control',
    transformData: function(hits) {
      return hits.map(function (hit) {
        hit.url = updateHitURL(hit);
        return hit;
      });
    }
  });
</script>
</body>
</html>
