{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Principal Component Analysis\nThis notebook introduces what is adaptive best subset selection principal component analysis (SparsePCA) and uses a real data example to show how to use it. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PCA\nPrincipal component analysis (PCA) is an important method in the field of data science, which can reduce the dimension of data and simplify our model. It actually solve an optimization problem like:\n\n..math::\n    \\max_{v} v^{\\top}\\Sigma v,\\qquad s.t.\\quad v^Tv=1.\n\n\nwhere $\\Sigma = X^TX / (n-1)$ and $X$ is the **centered** sample matrix. We also denote that $X$ is a $n\\times p$ matrix, where each row is an observation and each column is a variables.\n\nThen, before further analysis, we can project $X$ to $v$ (thus dimensional reduction), without losing too much information.\n\nHowever, consider that: \n\n- The PC is a linear combination of all primary variables ($Xv$), but sometimes we may tend to use less variables for clearer interpretation (and less computational complexity);\n- It has been proved that if $p/n$ does not converge to $0$, the classical PCA is not consistent, but this would happen in some high-dimensional data analysis.\n\n> For example, in gene analysis, the dataset may contain plenty of genes (variables) and we would like to find a subset of them, which can explain most information. Compared with using all genes, this small subset may perform better on interpretation, without loss much information. Then we can focus on these variables in the further analysis.\n\nWhen we trapped by these problems, a classical PCA may not be a best choice, since it use all variables. One of the alternatives is `SparsePCA`, which is able to seek for principal component with a sparsity limitation:\n\n..math::\n    \\max_{v} v^{\\top}\\Sigma v,\\qquad s.t.\\quad v^Tv=1,\\ ||v||_0\\leq s.\n\n\nwhere $s$ is a non-negative integer, which indicates how many primary variables are used in principal component. With `SparsePCA`, we can search for the best subset of variables to form principal component and it retains consistency even under $p>>n$. And we make two remarks: \n\n> Clearly, if $s$ is equal or larger than the number of primary variables, this sparsity limitation is actually useless, so the problem is equivalent to a classical PCA.\n\n> With less variables, the PC must have lower explained variance. However, this decrease is slight if we choose a good $s$ and at this price, we can interpret the PC much better. It is worthy. \n\nIn the next section, we will show how to form `SparsePCA`.\n\n## Real Data Example (Communities and Crime Dataset)\n\nHere we will use real data analysis to show how to form `SparsePCA`. The data we use is from [UCI:\nCommunities and Crime Data Set](https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime) and we pick up its 99 predictive variables as our samples.\n\nFirstly, we read the data and pick up those variables we interested.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom abess.decomposition import SparsePCA\n\nX = np.genfromtxt('communities.data', delimiter = ',') \nX = X[:, 5:127]                         # numeric predictiors\nX = X[:, ~np.isnan(X).any(axis = 0)]    # drop variables with nan\n\nn, p = X.shape\nprint(n)\nprint(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model fitting\nTo build an SparsePCA model, we need to give the target sparisty to its `support_size` argument. Our program supports adaptively finding a best sparisty in a given range. \n\nFixed sparsity\n \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nIf we only focus on one fixed sparsity, you can simply give a single integer to fit on this situation. And then the fitted sparse principal component is stored in `SparsePCA.coef_`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = SparsePCA(support_size = 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Give either $X$ or $\\Sigma$ to `model.fit()` and the fitting process will start. The argument `is_normal = False` here means that the program will not normalize $X$. Note that if both $X$ and $Sigma$ are given, the program prefer to use $X$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.fit(X = X, is_normal = False)\n# model.fit(Sigma = np.cov(X.T))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After fitting, `model.coef_` returns the sparse principal component and its non-zero positions correspond to variables used. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "temp = np.nonzero(model.coef_)[0]\nprint('sparsity: ', temp.size)\nprint('non-zero position: \\n', temp)\nprint(model.coef_.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Adaptive sparsity\nWhat's more, **abess** also support a range of sparsity and adaptively choose the best-explain one. However, usually a higher sparsity level would lead to better explaination.\n\nNow, you need to build an $s_{max} \\times 1$ binomial matrix, where $s_{max}$ indicates the max target sparsity and each row indicates one sparsity level (i.e. start from $1$, until $s_{max}$). For each position with $1$, **abess** would try to fit the model under that sparsity and finally give the best one.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# fit sparsity from 1 to 20\nsupport_size = np.ones((20, 1))\n# build model\nmodel = SparsePCA(support_size = support_size)\nmodel.fit(X, is_normal = False)  \n# results\ntemp = np.nonzero(model.coef_)[0]\nprint('chosen sparsity: ', temp.size)\nprint('non-zero position: \\n', temp)\nprint(model.coef_.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Because of warm-start, the results here may not be the same as fitted sparsity.*\n\nThen, the explained variance can be computed by:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Xc = X - X.mean(axis = 0)\nXv = Xc @ model.coef_\nexplained = Xv.T @ Xv                   # explained variance (information)\ntotal = sum(np.diag(Xc.T @ Xc))         # total variance (information)\nprint( 'explained ratio: ', explained / total )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### More on the results\nWe can give different target sparsity (change `s_begin` and `s_end`) to get different sparse loading. Interestingly, we can seek for a smaller sparsity which can explain most of the variance.\n\nIn this example, if we try sparsities from $0$ to $p$, and calculate the ratio of explained variance:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num = 30\ni = 0\nsparsity = np.linspace(1, p - 1, num, dtype='int')\nexplain = np.zeros(num)\nXc = X - X.mean(axis = 0)\nfor s in sparsity:\n    model = SparsePCA(\n        support_size = np.ones((s, 1)),\n        exchange_num = int(s),  \n        max_iter = 50\n    )\n    model.fit(X, is_normal = False)\n    Xv = Xc @ model.coef_\n    explain[i] = Xv.T @ Xv\n    i += 1\n\nprint('80%+ : ', sparsity[explain > 0.8 * explain[num-1]])\nprint('90%+ : ', sparsity[explain > 0.9 * explain[num-1]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we denote the explained ratio from all 99 variables as 100%, the curve indicates that at least 31 variables can reach 80% (blue dashed line) and 41 variables can reach 90% (red dashed line).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nplt.plot(sparsity, explain)\nplt.xlabel('Sparsity')\nplt.ylabel('Explained variance')\n\nind = np.where(explain > 0.8 * explain[num-1])[0][0]\nplt.plot([0, sparsity[ind]], [explain[ind], explain[ind]], 'b--')\nplt.plot([sparsity[ind], sparsity[ind]], [0, explain[ind]], 'b--')\nplt.text(sparsity[ind], 0, str(sparsity[ind]))\nplt.text(0, explain[ind], '80%')\n\nind = np.where(explain > 0.9 * explain[num-1])[0][0]\nplt.plot([0, sparsity[ind]], [explain[ind], explain[ind]], 'r--')\nplt.plot([sparsity[ind], sparsity[ind]], [0, explain[ind]], 'r--')\nplt.text(sparsity[ind], 0, str(sparsity[ind]))\nplt.text(0, explain[ind], '90%')\n\nplt.plot([0, p], [explain[num-1], explain[num-1]], color='gray', linestyle='--')\nplt.text(0, explain[num-1],'100%')\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This result shows that using less than half of all 99 variables can be close to perfect. For example, if we choose sparsity 31, the used variables are:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = SparsePCA(support_size = 31)\nmodel.fit(X, is_normal = False)\ntemp = np.nonzero(model.coef_)[0]\nprint('non-zero position: \\n', temp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extension: Group PCA\nGroup PCA\n^^^^^^^^^^^^^^^^^^^\nFurthermore, in some situation, some variables may need to consider together, that is, they should be \"used\" or \"unused\" for PC at the same time, which we call \"group information\". The optimization problem becomes:\n\n$`\n    \\max_{v} v^{\\top}\\Sigma v,\\qquad s.t.\\quad v^Tv=1,\\ \\sum_{g=1}^G I(||v_g||\\neq 0)\\leq s.\n:math:$`\n\nwhere we suppose there are $G$ groups, and the $g$-th one correspond to $v_g$, $v = [v_1^{\\top},v_2^{\\top},\\cdots,v_G^{\\top}]^{\\top}$. Then we are interested to find $s$ (or less) important groups.\n\n> Group problem is extraordinary important in real data analysis. Still take gene analysis as an example, several sites would be related to one charcter, and it is meaningless to consider each of them alone. \n\n`SparsePCA` can also deal with group information. Here we make sure that variables in the same group address close to each other (if not, the data should be sorted first).\n\n### Simulated Data Example\nSuppose that the data above have group information like:\n\n- Group 0: {the 1st, 2nd, ..., 6th variable};\n- Group 1: {the 7th, 8th, ..., 12th variable};\n- ...\n- Group 15: {the 91st, 92nd, ..., 96th variable};\n- Group 16: {the 97th, 98th, 99th variables}.\n\nDenote different groups as different number:  \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "g_info = np.arange(17) \ng_info = g_info.repeat(6)\ng_info = g_info[0:99]\n\nprint(g_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And fit a group sparse PCA model with additional argument `group=g_info`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = SparsePCA(support_size = np.ones((6, 1)))\nmodel.fit(X, group = g_info, is_normal = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result comes to:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(model.coef_.T)\n\ntemp = np.nonzero(model.coef_)[0]\ntemp = np.unique(g_info[temp])\n\nprint('non-zero group: \\n', temp)\nprint('chosen sparsity: ', temp.size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Hence we can focus on variables in Group 0, 8, 9, 10, 11, 15.\n Extension: Multiple principal components\n --------------------------------------------\n  Multiple principal components\n ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n In some cases, we may seek for more than one principal components under sparsity. Actually, we can iteratively solve the largest principal component and then mapping the covariance matrix to its orthogonal space:\n\n $`\n \\Sigma' = (1-vv^{\\top})\\Sigma(1-vv^{\\top})\n :math:$`\n\n where $\\Sigma$ is the currect covariance matrix and $v$ is its (sparse) principal component. We map it into $\\Sigma'$, which indicates the orthogonal space of $v$, and then solve the sparse principal component again. \n\n By this iteration process, we can acquire multiple principal components and they are sorted from the largest to the smallest.\n In our program, there is an additional argument `number`, which indicates the number of principal components we need, defaulted by 1.\n Now the `support_size` is shaped in $s_{max}\\times \\text{number}$ and each column indicates one principal component.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = SparsePCA(support_size = np.ones((31, 3)))\nmodel.fit(X, is_normal = False, number = 3)\nmodel.coef_.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, each column of the `model.coef_` is a sparse PC (from the largest to the smallest), for example the second one is that:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.coef_[:,1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we want to compute the explained variance of them, it is also quite easy: \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Xv = Xc.dot(model.coef_)\nexplained = np.sum(np.diag(Xv.T.dot(Xv)))\nprint( 'explained ratio: ', explained / total )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## R tutorial\nFor R tutorial, please view [https://abess-team.github.io/abess/articles/v08-sPCA.html](https://abess-team.github.io/abess/articles/v08-sPCA.html).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}