{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Linear Regression\n\nIn this tutorial, we are going to demonstrate how to use the `abess` package to carry out best subset selection \nin linear regression with both simulated data and real data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear Regression\n\nOur package `abess` implement a polynomial algorithm in the for best-subset selection problem:\n\n\\begin{align}\\min_{\\beta\\in \\mathbb{R}^p} \\frac{1}{2n} ||y-X\\beta||^2_2,\\quad \\text{s.t.}\\ ||\\beta||_0\\leq s,\\end{align}\n\n\nwhere $\\| \\cdot \\|_2$ is the $\\ell_2$ norm, $\\|\\beta\\|_0=\\sum_{i=1}^pI( \\beta_i\\neq 0)$ is the $\\ell_0$ norm of $\\beta$, and the sparsity level $s$ is usually an unknown non-negative integer.\nNext, we present an example to show how to use the `abess` package to solve a simple problem. \n\n### Simulated Data Example\nFixed Support Size Best Subset Selection\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nWe generate a design matrix $X$ containing 300 observation and each observation has 1000 predictors. The response variable $y$ is linearly related to the first, second, and fifth predictors in $X$:\n .. math::\n  y = 3X_1 + 1.5X_2 + 2X_5 + \\epsilon,\n\nwhere $\\epsilon$ is a standard normal random variable. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom abess.datasets import make_glm_data\nnp.random.seed(0)\n\nn = 300\np = 1000\nk = 3\nreal_coef = np.zeros(p)\nreal_coef[[0, 1, 4]] = 3, 1.5, 2\ndata1 = make_glm_data(n = n, p = p, k = k, family = \"gaussian\", coef_ = real_coef)\n\n\nprint(data1.x.shape)\nprint(data1.y.shape)\n\n# Use `LinearRegression` to fit the data, with a fixed support size:\n\nfrom abess import LinearRegression\nmodel = LinearRegression(support_size = 3)\nmodel.fit(data1.x, data1.y)\n\n\n# After fitting, the predicted coefficients are stored in `model.coef_`:\n\nprint(\"shape:\", model.coef_.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ind = np.nonzero(model.coef_)\nprint(\"predicted non-zero: \", ind)\nprint(\"predicted coef: \", model.coef_[ind])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the result, we know that `abess` found which 3 predictors are useful among all 1000 variables. Besides, the predicted coefficients of them are quite close to the real ones. \n\n#### Adaptive Best Subset Selection\nHowever, we may not know the true sparsity level in real world data, and thus we need to determine the most proper one from a large range. Suppose that we believe the real sparsity level is between 0 to 30 (so that `range(0, 31)`):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = LinearRegression(support_size = range(31))\nmodel.fit(data1.x, data1.y)\n\nind = np.nonzero(model.coef_)\nprint(\"predicted non-zero: \", ind)\nprint(\"predicted coef: \", model.coef_[ind])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The program can adaptively choose the sparsity level that best fit the data. It is not surprising that it choose 3 variables, the same as the last section. \n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Real data example\n\n### Hitters Dataset\nNow we focus on real data on the `Hitters` dataset: [https://www.kaggle.com/floser/hitters](https://www.kaggle.com/floser/hitters).\nWe hope to use sevral predictors related to the performance of the baseball atheltes last year to predict their salary.\n\nFirst, let's have a look at this dataset. There are 19 variables except `Salary` and 322 observations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\nimport os\n\ndata2 = pd.read_csv(os.path.join(os.getcwd(), 'Hitters.csv'))\nprint(data2.shape)\n\n\nprint(data2.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the dataset contains some missing values, we simply drop those rows with missing values. Then we have 263 observations remains:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data2 = data2.dropna()\nprint(data2.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What is more, before fitting, we need to transfer the character variables to dummy variables: \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data2 = pd.get_dummies(data2)\ndata2 = data2.drop(['League_A', 'Division_E', 'NewLeague_A'], axis = 1)\nprint(data2.shape)\nprint(data2.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Fitting\nAs what we do in simulated data, an adaptive best subset can be formed easily:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = np.array(data2.drop('Salary', axis = 1))\ny = np.array(data2['Salary'])\n\nmodel = LinearRegression(support_size = range(20))\nmodel.fit(x, y)\n\n\n# The result can be showed:\n\n\nind = np.nonzero(model.coef_)\nprint(\"non-zero:\\n\", data2.columns[ind])\nprint(\"coef:\\n\", model.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Automatically, variables $Hits$, $CRBI$, $PutOuts$, $League\\_N$ are chosen in the model (the chosen sparsity level is 4).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### More on the results\nWe can also plot the path of abess process:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\ncoef = np.zeros((20, 19))\nic = np.zeros(20)\nfor s in range(20):\n    model = LinearRegression(support_size = s)\n    model.fit(x, y)\n    coef[s, :] = model.coef_\n    ic[s] = model.ic_\n\nfor i in range(19):\n    plt.plot(coef[:, i], label = i)\n\nplt.xlabel('support_size')\nplt.ylabel('coefficients')\n# plt.legend() # too long to plot\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Besides, we can also generate a graph about the tuning value. Remember that we used the default EBIC to tune the support size.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.plot(ic, 'o-')\nplt.xlabel('support_size')\nplt.ylabel('EBIC')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In EBIC criterion, `support_size = 4` has the lowest value, so the process adaptively choose 4 variables. Note that under other information criterion, the result may be different. \n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### R tutorial \nFor R tutorial, please view [https://abess-team.github.io/abess/articles/v01-abess-guide.html](https://abess-team.github.io/abess/articles/v01-abess-guide.html).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}