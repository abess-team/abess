{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Logistic Regression and Multinomial Extension\n# We would like to use an example to show how the best subset selection for logistic regression work in our program.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Real Data Example\nTitanic Dataset\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nConsider the Titanic dataset obtained from the Kaggle competition: https://www.kaggle.com/c/titanic/data. \nThe dataset consists of data about 889 passengers, and the goal of the competition is to predict the survival (yes/no) based on features including the class of service, the sex, the age etc. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np \nimport pandas as pd\n\ndt = pd.read_csv(\"./train.csv\")\nprint(dt.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We only focus on some numeric or classification variables:\n\n- predictor variables: $Pclass,\\ Sex,\\ Age,\\ SibSp,\\ Parch,\\ Fare,\\ Embarked$;\n- response variable is $Survived$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dt = dt.iloc[:, [1,2,4,5,6,7,9,11]] # variables interested\ndt['Pclass'] = dt['Pclass'].astype(str)\nprint(dt.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, some rows contain missing value (NaN) and we need to drop them.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dt = dt.dropna()\nprint('sample size: ', dt.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then use dummy variables to replace classification variables:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dt1 = pd.get_dummies(dt)\nprint(dt1.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we split `dt1` into training set and testing set:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\nX = np.array(dt1.drop('Survived', axis = 1))\nY = np.array(dt1.Survived)\n\ntrain_x, test_x, train_y, test_y = train_test_split(X, Y, test_size = 0.33, random_state = 0)\nprint('train size: ', train_x.shape[0])\nprint('test size:', test_x.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here `train_x` contains:\n\n- V0: dummy variable, 1st ticket class (1-yes, 0-no)\n- V1: dummy variable, 2nd ticket class (1-yes, 0-no)\n- V2: dummy variable, sex (1-male, 0-female)\n- V3: Age\n- V4: # of siblings / spouses aboard the Titanic\n- V5: # of parents / children aboard the Titanic\n- V6: Passenger fare\n- V7: dummy variable, Cherbourg for embarkation (1-yes, 0-no)\n- V8: dummy variable, Queenstown for embarkation (1-yes, 0-no)\n\nAnd `train_y` indicates whether the passenger survived (1-yes, 0-no).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('train_x:\\n', train_x[0:5, :])\nprint('train_y:\\n', train_y[0:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Fitting\nThe `LogisticRegression()` function in the `abess.linear` allows you to perform best subset selection in a highly efficient way. For example, in the Titanic sample, if you want to look for a best subset with no more than 5 variables on the logistic model, you can call:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from abess.linear import LogisticRegression\n\ns = 5   # max target sparsity\nmodel = LogisticRegression(support_size = range(0, s + 1))\nmodel.fit(train_x, train_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now the `model.coef_` contains the coefficients of logistic model with no more than 5 variables. That is, those variables with a coefficient 0 is unused in the model: \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(model.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By default, the `LogisticRegression` function set the `support_size = range(0, min(p,n/log(n)p)` and the best support size is determined by theExtended Bayesian Information Criteria (EBIC). You can change the tunging criterion by specifying the argument `ic_type`. The available tuning criterion now are `gic`, `aic`, `bic`, `ebic`. \n\nFor a quicker solution, you can change the tuning strategy to a golden section path which trys to find the elbow point of the tuning criterion over the hyperparameter space. Here we give an example.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_gs = LogisticRegression(path_type = \"gs\", s_min = 0, s_max = s)\nmodel_gs.fit(train_x, train_y)\nprint(model_gs.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "where `s_min` and `s_max` bound the support size and this model give the same answer as before.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# More on the Results\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n# After fitting with `model.fit()`, we can further do more exploring work to interpret it. \n# As we show above, `model.coef_` contains the sparse coefficients of variables and those non-zero values indicates \"important\" varibles chosen in the model.\n\n\nprint('Intercept: ', model.intercept_)\nprint('coefficients: \\n', model.coef_)\nprint('Used variables\\' index:', np.nonzero(model.coef_ != 0)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training loss and the score under information criterion:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('Training Loss: ', model.train_loss_)\nprint('IC: ', model.ic_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prediction is allowed for the estimated model. Just call `model.predict()` function like: \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fitted_y = model.predict(test_x)\nprint(fitted_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Besides, you can also call for the survival probability of each observation by `model.predict_proba()`. Actually, those who with a probability greater than 0.5 is classified to \"1\" (survived).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fitted_p = model.predict_proba(test_x)\nprint(fitted_p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also generate an ROC curve and calculate tha AUC value. On this dataset, the AUC is 0.817, which is quite close to 1.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\nfpr, tpr, _ = roc_curve(test_y, fitted_p)\nplt.plot(fpr, tpr)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.show()\n\nprint('AUC: ', auc(fpr, tpr))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extension: Multi-class Classification\n------------------------------------------- \nMultinomial logistic regression\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nWhen the number of classes is more than 2, we call it multi-class classification task. Logistic regression can be extended to model several classes of events such as determining whether an image contains a cat, dog, lion, etc. Each object being detected in the image would be assigned a probability between 0 and 1, with a sum of one. The extended model is multinomial logistic regression.\n\nTo arrive at the multinomial logistic model, one can imagine, for $K$ possible classes, running $K\u22121$ independent logistic regression models, in which one class is chosen as a \"pivot\" and then the other $K\u22121$ classes are separately regressed against the pivot outcome. This would proceed as follows, if class K (the last outcome) is chosen as the pivot:\n\n..math::\n    \\ln (\\mathbb{P}(y=1)/\\mathbb{P}(y=K)) = x^T\\beta^{(1)},\\\\\n    \\dots\\ \\dots\\\\\n    \\ln (\\mathbb{P}(y=K-1)/\\mathbb{P}(y=K)) = x^T\\beta^{(K-1)}.\n\n\nThen, the probability to choose the j-th class can be easily derived to be:\n\n..math::\n    \\mathbb{P}(y=j) = \\frac{\\exp(x^T\\beta^{(j)})}{1+\\sum_{k=1}^{K-1} \\exp(x^T\\beta^{(k)})},\n\n\nand subsequently, we would predict the $j^*$-th class if the $j^*=\\arg\\max_j \\mathbb{P}(y=j)$. Notice that, for $K$ possible classes case, there are $p\\times(K\u22121)$ unknown parameters: $\\beta^{(1)},\\dots,\\beta^{(K\u22121)}$ to be estimated. Because the number of parameters increase as $K$, it is even more urge to constrain the model complexity. And the best subset selection for multinomial logistic regression aims to maximize the log-likelihood function and control the model complexity by restricting $B=(\\beta^{(1)},\\dots,\\beta^{(K\u22121)})$ with $||B||_{0,2}\\leq s$ where $||B||_{0,2}=\\sum_{i=1}^p I(B_{i\\cdot}=0)$, $B_{i\\cdot}$ is the $i$-th row of coefficient matrix $B$ and $0\\in R^{K-1}$ is an all zero vector. In other words, each row of $B$ would be either all zero or all non-zero.\n\n### Simulated Data Example\n\nWe shall conduct Multinomial logistic regression on an artificial dataset for demonstration. The `make_multivariate_glm_data()` provides a simple way to generate suitable for this task. \n\nThe assumption behind is the response vector following a multinomial distribution. The artifical dataset contain 100 observations and 20 predictors but only five predictors have influence on the three possible classes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from abess.datasets import make_multivariate_glm_data\nn = 100 # sample size\np = 20  # all predictors\nk = 5   # real predictors\nM = 3   # number of classes\n\nnp.random.seed(0)\ndt =  make_multivariate_glm_data(n = n, p = p, k = k, family = \"multinomial\", M = M)\nprint(dt.coef_)\nprint('real variables\\' index:\\n', set(np.nonzero(dt.coef_)[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To carry out best subset selection for multinomial logistic regression, we can call the `MultinomialRegression()`. Here is an example.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from abess.linear import MultinomialRegression\ns = 5\nmodel = MultinomialRegression(support_size = range(0, s + 1))\nmodel.fit(dt.x, dt.y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Its use is quite similar to `LogisticRegression`. We can get the coefficients to recognize \"in-model\" variables.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('intercept:\\n', model.intercept_)\nprint('coefficients:\\n', model.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So those variables used in model can be recognized and we ca find that they are the same as the data's \"real\" coefficients we generate.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('used variables\\' index:\\n', set(np.nonzero(model.coef_)[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## R tutorial\nFor R tutorial, please view [https://abess-team.github.io/abess/articles/v03-classification.html](https://abess-team.github.io/abess/articles/v03-classification.html).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}