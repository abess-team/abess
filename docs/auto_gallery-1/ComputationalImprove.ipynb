{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Computational Improvement\n The generic splicing technique certifiably guarantees the best subset can be selected in a polynomial time. In practice, the computational efficiency can be improved to handle large scale datasets. The tips for computational improvement include:\n \n - exploit sparse strucute of input matrix;\n - use golden-section to search best support size;\n - focus on important variables when splicing;\n - early-stop scheme;\n - sure independence screening;\n - warm-start initialization;\n - parallel computing when performing cross validation;\n - covariance update for `LinearRegression` or `MultiTaskRegression`;\n - approximate Newton iteration for `LogisticRegression`, `PoissonRegression`, `CoxRegression`.\n \n This vignette illustrate the first two tips. For the other tips, they have been efficiently implemented and set as the default in abess package.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sparse matrix\n\nWe sometimes meet with problems where the $N\u00d7p$ input matrix $X$ is extremely sparse, i.e., many entries in $X:math:$ have zero values. A notable example comes from document classification: aiming to assign classes to a document, making it easier to manage for publishers and news sites. The input variables for characterizing documents are generated from a so called \"bag-of-words\" model. In this model, each variable is scored for the presence of each of the words in the entire dictionary under consideration. Since most words are absent, the input variables for each document is mostly zero, and so the entire matrix is mostly zero. \n\nFor example, we create a sparse matrix like:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import coo_matrix\nimport numpy as np\n\nrow  = np.array([0, 1, 2, 3, 4, 4,  5, 6, 7, 7, 8, 9])\ncol  = np.array([0, 3, 1, 2, 4, 3, 5, 2, 3, 1, 5, 2])\ndata = np.array([4, 5, 7, 9, 1, 23, 4, 5, 6, 8, 77, 100])\nx = coo_matrix((data, (row, col)))\n\nprint(x.toarray())\n\n##%\n# The sparse matrix can be directly used in `abess` pacakages. We just need to set argument `sparse_matrix = T`. Note that if the input matrix is not sparse matrix, the program would automatically transfer it into the sparse one, so this argument can also make some improvement.\n\n\nfrom abess import LinearRegression\n\ncoef = np.array([1, 1, 1, 0, 0, 0])\ny = x.dot(coef)\nmodel = LinearRegression(sparse_matrix = True)\nmodel.fit(x, y)\n\nprint(\"real coef: \\n\", coef)\nprint(\"pred coef: \\n\", model.coef_)\n\n##%\n# We compare the runtime when the input matrix is dense matrix:\n\n\nfrom time import time\n\nt = time()\nmodel = LinearRegression()\nmodel.fit(x.toarray(), y)\nprint(\"dense matrix:  \", time() - t)\n\nt = time()\nmodel = LinearRegression(sparse_matrix = True)\nmodel.fit(x, y)\nprint(\"sparse matrix:  \", time() - t)\n\n##%\n# From the comparison, we see that the time required by sparse matrix is smaller, and this sould be more visible when the sparse imput matrix is large. Hence, we suggest to assign a sparse matrix to `abess` when the input matrix have a lot of zero entries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Golden-section searching\nHere we generate a simple example and draw the path of scores of information criterion. Typically, the curve should be a strictly unimodal function achieving minimum at the true subset size.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nfrom abess.datasets import make_glm_data\n\nnp.random.seed(0)\ndata = make_glm_data(n = 100, p = 20, k = 5, family = 'gaussian')\n\nic = np.zeros(21)\nfor sz in range(21):\n    model = LinearRegression(support_size = [sz], ic_type = 'ebic')\n    model.fit(data.x, data.y)\n    ic[sz] = model.ic_\n\nprint(\"lowest point: \", np.argmin(ic))\nplt.plot(ic, 'o-')\nplt.xlabel('support_size')\nplt.ylabel('EBIC')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here the generated data contains 100 observations with 20 predictors, while 5 of them are useful (should be non-zero). The default information criterion is EBIC. From the figure, we can find that \"support_size = 5\" is the lowest point.\n\nCompared with searching the optimal support size one by one from a candidate set with $O(s_{max})$ complexity, **golden-section** reduce the time complexity to $O(ln(s_{max}))$, giving a significant computational improvement.\n\nIn `abess` package, this can be easily formed like:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = LinearRegression(path_type = 'gs', s_min = 0, s_max = 20)\nmodel.fit(data.x, data.y)\nprint(\"real coef:\\n\", np.nonzero(data.coef_)[0])\nprint(\"predicted coef:\\n\", np.nonzero(model.coef_)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "where `path_type = gs` means golden-section and `s_min`, `s_max` indicates the left and right bound of range of the support size. Note that in golden-section searching, we should not give `support_size`, which is only useful for sequential strategy.\n\nThe output of golden-section strategy suggests the optimal model size is accurately detected. Compare to the sequential searching, the golden section reduce the runtime because it skip some support sizes which are likely to be a non-optimal one:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from time import time\n\nt1 = time()\nmodel = LinearRegression(support_size = range(21))\nmodel.fit(data.x, data.y)\nprint(\"sequential time: \", time() - t1)\n\nt2 = time()\nmodel = LinearRegression(path_type = 'gs', s_min = 0, s_max = 20)\nmodel.fit(data.x, data.y)\nprint(\"golden-section time: \", time() - t2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The golden-section runs much faster than sequential method, espectially when the range of support size is large.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Important Search\nSuppose that there are only a few variables are important (i.e. too many noise variables), it may be a vise choice to focus on some important variables in splicing process. This can save a lot of time, especially under a large $p$.\n\nIn abess package, an argument called `important_search` is used for it, which means the size of inactive set for each splicing process. By default, this argument is set as 0, and the total inactive variables would be contained in the inactive set. But if an positive integer is given, the splicing process would focus on active set and the most important `important_search` inactive variables.\n\nHowever, after convergence on this subset, we check if the chosen variables are still the most important ones by recomputing on the full set with the new active set. If not, we update the subset and splicing again. On our testing, it would not iterate many time to reach a stable subset. After that, the active set on the stable subset would be treated as that on the full set.\n\nHere we take `LogisticRegression` for an example. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from abess.linear import LogisticRegression\nfrom abess.datasets import make_glm_data\nfrom time import time\nimport numpy as np\n\ndata = make_glm_data(n = 500, p = 10000, k = 10, family = \"binomial\")\n\nt1 = time()\nmodel = LogisticRegression()\nmodel.fit(data.x, data.y)\nt2 = time()\n\nprint(\"non_zero :\\n\", np.nonzero(model.coef_)[0])\nprint(\"time : \", t2 - t1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, if we only focus on 500 important inactive variables when searching:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t1 = time()\nmodel2 = LogisticRegression(important_search = 500)\nmodel2.fit(data.x, data.y)\nt2 = time()\n\nprint(\"non_zero :\\n\", np.nonzero(model2.coef_)[0])\nprint(\"time : \", t2 - t1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It takes much less time to reach the same result. We recommend use this method for large $p$ situation, but in small one, it may not be faster than the primary fitting.\n\nHere we compare the AUC and runtime for `LogisticRegression` under different `important_search` and the test code can be found [here](https://github.com/abess-team/abess/blob/master/docs/simulation/Python/impsearch.py).\n\n![](./fig/impsearch.png)\n\nAt a low level of `important_search`, however, the performance (AUC) has been very good. In this situation, a lower `important_search` can save lots of time and space.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## R tutorial\n\nFor R tutorial, please view [https://abess-team.github.io/abess/articles/v09-fasterSetting.html](https://abess-team.github.io/abess/articles/v09-fasterSetting.html).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}