{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nRobust Principal Component Analysis\n======================================== \nThis notebook introduces what is adaptive best subset selection robust principal component analysis (RobustPCA) and then we show how it works using **abess** package on an artificial example.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PCA\nPrincipal component analysis (PCA) is an important method in the field of data science, which can reduce the dimension of data and simplify our model. It solves an optimization problem like:\n\n..math::\n    \\max_{v} v^T\\Sigma v,\\qquad s.t.\\quad v^Tv=1.\n\n\nwhere $\\Sigma = X^TX/(n-1)$ and $X\\in \\mathbb{R}^{n\\times p}$ is the centered sample matrix with each row containing one observation of $p$ variables.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Robust-PCA (RPCA)\nHowever, the original PCA is sensitive to outliers, which may be unavoidable in real data:\n\n- Object has extreme performance due to fortuity, but he/she shows normal in repeated test;\n- Wrong observation/recording/computing, e.g. missing or dead pixels, X-ray spikes.\n\nIn this situation, PCA may spend too much attention on unnecessary variables. \nThat's why Robust-PCA (RPCA) is presented, which can be used to recover the (low-rank) sample for subsequent processing.\n\nIn mathematics, RPCA manages to divide the sample matrix $X$ into two parts: \n\n..math::\n    X = S + L, \n\n\nwhere $S$ is the sparse \"outlier\" matrix and $L$ is the \"information\" matrix with a low rank. \nGenerally, we also suppose $S$ is not low-rank and $L$ is not sparse, in order to get unique solution.\n\n![](./fig/rpca.png)\n\nIn Lagrange format, \n\n..math::\n    \\min _{S, L}\\|X-S-L\\|_{F} \\leq \\varepsilon, s . t . \\quad \\operatorname{rank}(L)=r,\\|S\\|_{0} \\leq s\n\n\nwhere $s$ is the sparsity of $S$.\nAfter RPCA, the information matrix $L$ can be used in further analysis.\n\n> Note that it does NOT deal with \"noise\", which may stay in $L$ and need further procession.  \n\n# Hard Impute\nTo solve its sub-problem, RPCA under known outlier positions, we follow a process called \"Hard Impute\".\nThe main idea is to estimate the outlier values by precise values with KPCA, where $K=r$.\n\nHere are the steps:\n\n1. Input $X, outliers, M, \\varepsilon$, where $outliers$ records the non-zero positions in $S$; \n\n2. Denote $X_{\\text{new}} \\leftarrow {\\bf 0}$ with the same shape of $X$;\n\n3. For $i = 1,2, \\dots, M$:\n\n   - $X_{\\text{old}} = \\begin{cases} X_{\\text{new}},&\\text{for } outliers\\\\X,&\\text{for others}\\end{cases}$;\n\n   - Form KPCA on $X_{\\text{old}}$ with $K=r$, and denote $v$ as the eigenvectors;\n\n   - $X_{\\text{new}} = X_{\\text{old}}\\cdot v\\cdot v^T$;\n\n   - If $\\|X_{\\text{new}} - X_{\\text{old}}\\| < \\varepsilon$, break;\n\n   End for;\n\n4. Return $X_{\\text{new}}$ as $L$;\n\nwhere $M$ is the maximum iteration times and $\\varepsilon$ is the convergence coefficient.\n\nThe final $X_{\\text{new}}$ is supposed to be $L$ under given outlier positions.\n\n## RPCA Application\n\nRecently, RPCA is more widely used, for example,\n\n- Video Decomposition: \nin a surveillance video, the background may be unchanged for a long time while only a few pixels (e.g. people) update. \nIn order to improve the efficiency of store and analysis, we need to decomposite the video into background and \nforeground. Since the background is unchanged, it can be stored well in a low-rank matrix, while the foreground, which is \nusually quite small, can be indicated by a sparse matrix. That is what RPCA does.\n\n- Face recognition: \ndue to complex lighting conditions, a small part of the facial features may be unrecognized (e.g. shadow).\nIn the face recognition, we need to remove the effects of shadows and focus on the face data. Actually, since the face data is almost unchanged (for one person), and the shadows affect only a small part, it is also a suitable situation to use RPCA. Here are some examples: \n\n![](./fig/rpca_shadow.png)\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simulated Data Example\nFitting model\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nNow we generate an example with $100$ rows and $100$ columns with $200$ outliers.\nWe are looking forward to recover it with a low rank $10$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\ndef gen_data(n, p, s, r, seed = 0):\n    np.random.seed(seed)\n    outlier = np.random.choice(n*p, s, replace=False)\n    outlier = np.vstack((outlier//p, outlier%p)).T\n    L = np.dot(np.random.rand(n, r), np.random.rand(r, n))\n    S = np.zeros((n, p))\n    S[outlier[:, 0], outlier[:, 1]] = float(np.random.randn(1)) * 10\n    X = L + S\n    return X, S\n\nn = 100     # rows\np = 100     # columns\ns = 200     # outliers\nr = 10      # rank(L)\n\nX, S = gen_data(n, p, s, r)\nprint(f'X shape: {X.shape}')\n# print(f'outlier: \\n{outlier}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to use our program, users should call `RobustPCA()` and give the outlier number to `support_size`. Note that it can be a specific integer or an integer interval. For the latter case, a support size will be chosen by information criterion (e.g. GIC) adaptively.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from abess.decomposition import RobustPCA\nmodel = RobustPCA(support_size = s) # support_size can be a interval like `range(s_min, s_max)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is quite easy to fit this model, with `RobustPCA.fit` function. Given the original sample matrix $X$ and $rank(L)$ we wnat, the program will give a result quickly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.fit(X, r = r) # r=rank(L)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now the estimated outlier matrix is stored in `model.coef_`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "S_est = model.coef_\nprint(f'estimated sparsity: {np.count_nonzero(S_est)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## More on the result\nTo check the performance of the program, we use TPR, FPR as the criterion.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def TPR(pred, real):\n    TP = (pred != 0) & (real != 0)\n    P = (real != 0)\n    return sum(sum(TP)) / sum(sum(P))\n\ndef FPR(pred, real):\n    FP = (pred != 0) & (real == 0)\n    N = (real == 0)\n    return sum(sum(FP)) / sum(sum(N))\n\ndef test_model(pred, real):\n    tpr = TPR(pred, real)\n    fpr = FPR(pred, real)\n    return np.array([tpr, fpr])\n\nprint(f'[TPR  FPR] = {test_model(S_est, S)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also change different random seed to test for more situation:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "M = 30  # use 30 different seed\nres = np.zeros(2)\nfor seed in range(M):\n    X, S = gen_data(n, p, s, r, seed)\n    model = RobustPCA(support_size=s).fit(X, r=r)\n    res += test_model(model.coef_, S)\n\nprint(f'[TPR  FPR] = {res/M}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Under all of these situations, `RobustPCA` have a good performance.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}